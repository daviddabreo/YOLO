{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the custom-trained YOLOv5 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video stream.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"Webcam successfully opened!\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Run detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Annotate the frame with bounding boxes and labels using the plot() method\n",
    "    frame = results[0].plot()  # This will display the bounding boxes directly on the frame\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"Fruit Detection\", frame)\n",
    "\n",
    "    # Exit when 'ESC' is pressed\n",
    "    if cv2.waitKey(1) == 27:  # ESC key to exit\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 1:\n",
      "\n",
      "✅ Model runs/detect/cars_1_scratch/weights/best.pt - Total cars crossed line: 23\n",
      "\n",
      "\n",
      "✅ Model runs/detect/cars_0_1_transfer_freeze/weights/best.pt - Total cars crossed line: 19\n",
      "\n",
      "Video 2:\n",
      "\n",
      "✅ Model runs/detect/cars_1_scratch/weights/best.pt - Total cars crossed line: 22\n",
      "\n",
      "\n",
      "✅ Model runs/detect/cars_0_1_transfer_freeze/weights/best.pt - Total cars crossed line: 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "\n",
    "# Define your models\n",
    "\"\"\"\n",
    "models = [\n",
    "    \"runs/detect/cars_0_scratch/weights/best.pt\",\n",
    "]\n",
    "\"\"\"\n",
    "videos = [\"vid4.mp4\",\"vid5.mp4\" ]#, \"vid3.mp4\", \"vid4.mp4\", \"vid5.mp4\"]\n",
    "for n, video in enumerate(videos):\n",
    "#if(True):\n",
    "    #n = 4\n",
    "    print(f\"Video {n+1}:\")\n",
    "    models = [ [\n",
    "        \"runs/detect/cars_1_scratch/weights/best.pt\",\n",
    "        \"runs/detect/cars_0_1_transfer_freeze/weights/best.pt\"\n",
    "        ],\n",
    "\n",
    "        [\"runs/detect/cars_0_scratch/weights/best.pt\",\n",
    "        \"runs/detect/cars_1_scratch/weights/best.pt\",\n",
    "    \"runs/detect/cars_0_1_transfer/weights/best.pt\",\n",
    "    \"runs/detect/cars_0_1_transfer_freeze/weights/best.pt\"\n",
    "        ] ]\n",
    "    \n",
    "    crossing_line = [[300, 950, 950, 450, 550],[450, 975, 975, 575 , 675], [600, 1000, 1000, 700, 800]]\n",
    "\n",
    "    # Line y-coordinate\n",
    "    crossing_line_y = crossing_line[0][n+3]\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # Get video properties\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Initialize video writer (output video file)\n",
    "    video_writer = cv2.VideoWriter(\"output_video.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "    # Loop through each model and evaluate\n",
    "    for model_path in models[0]:\n",
    "        model = YOLO(model_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # rewind video\n",
    "        crossed_ids = set()\n",
    "        car_count = 0  # Counter for cars crossing the line\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Resize the frame to a smaller resolution\n",
    "            resize_width = 1920  # Target width\n",
    "            resize_height = 1080  # Target height\n",
    "            frame = cv2.resize(frame, (resize_width, resize_height))\n",
    "            results = model.track(frame, persist=True, conf=0.7, iou=0.5, tracker=\"bytetrack.yaml\", verbose=False)\n",
    "\n",
    "            # if results[0].boxes.id is None:\n",
    "            #     continue\n",
    "            annotated_frame = frame\n",
    "            if results[0].boxes.id is not None:\n",
    "                ids = results[0].boxes.id.cpu().tolist()\n",
    "                boxes = results[0].boxes.xyxy.cpu().tolist()\n",
    "                annotated_frame = results[0].plot()\n",
    "                \n",
    "\n",
    "                for track_id, (x1, y1, x2, y2) in zip(ids, boxes):\n",
    "                    cx = (x1 + x2) / 2\n",
    "                    cy = (y1 + y2) / 2\n",
    "\n",
    "                    if track_id not in crossed_ids and cy < crossing_line_y:\n",
    "                        crossed_ids.add(track_id)\n",
    "                        car_count += 1  # Increment car count\n",
    "                        #print(f\"Model {model_path}: Track ID {track_id} crossed.\")\n",
    "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the crossing line\n",
    "            cv2.line(annotated_frame, (0, crossing_line_y), (annotated_frame.shape[1], crossing_line_y), (0, 255, 255), 2)\n",
    "            # Display car count on frame\n",
    "            cv2.putText(annotated_frame, f'Cars Crossed: {car_count}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Show frame with line and car count\n",
    "            cv2.imshow(f\"{model_path}\", annotated_frame)\n",
    "\n",
    "            # Write to video output\n",
    "            video_writer.write(annotated_frame)\n",
    "\n",
    "            # Slow down the video by increasing the wait time (delay between frames)\n",
    "            if cv2.waitKey(5) == 27:  # Increase wait time from default 1 to 100ms\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "        print(f\"\\n✅ Model {model_path} - Total cars crossed line: {car_count}\\n\")\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "# Release resources\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.102  Python-3.11.5 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\VsCode\\Computer_Vision\\CNN.YOLO\\car_dataset\\labels\\test.cache... 49 images, 0 backgrounds, 0 corrupt: 100%|██████████| 49/49 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         49        317      0.828      0.902      0.924      0.769\n",
      "Speed: 0.5ms preprocess, 3.5ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Ultralytics 8.3.102  Python-3.11.5 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\VsCode\\Computer_Vision\\CNN.YOLO\\car_dataset\\labels\\test.cache... 49 images, 0 backgrounds, 0 corrupt: 100%|██████████| 49/49 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         49        317      0.827      0.849       0.91      0.762\n",
      "Speed: 1.1ms preprocess, 5.9ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n",
      "Ultralytics 8.3.102  Python-3.11.5 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\VsCode\\Computer_Vision\\CNN.YOLO\\car_dataset\\labels\\test.cache... 49 images, 0 backgrounds, 0 corrupt: 100%|██████████| 49/49 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         49        317      0.821      0.849      0.891      0.724\n",
      "Speed: 0.6ms preprocess, 7.2ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n",
      "pretrained and fine tuned (20 epochs) \n",
      "Accuracy: 0.77\n",
      "Scratch (5 epochs) \n",
      "Accuracy: 0.76\n",
      "Scratch (15 epochs) \n",
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "#pre trained without fine tuning.\n",
    "#model1 = YOLO('runs/detect/cars_0_scratch/weights/best.pt')  # load a pretrained model\n",
    "model2 = YOLO('runs/detect/cars_1_scratch/weights/best.pt')\n",
    "model3 = YOLO('runs/detect/cars_0_1_transfer/weights/best.pt')\n",
    "model4 = YOLO('runs/detect/cars_0_1_transfer_freeze/weights/best.pt')\n",
    "\n",
    "#result1 = model1.val(data='data/traffic.yaml', split = 'test')\n",
    "result2 = model2.val(data='car_dataset/data.yaml', split = 'test') \n",
    "result3 = model3.val(data='car_dataset/data.yaml', split = 'test')  \n",
    "result4 = model4.val(data='car_dataset/data.yaml', split = 'test') \n",
    "#print(f'pretrained model \\nAccuracy: {result1.box.map:.2f}')\n",
    "print(f'pretrained and fine tuned (20 epochs) \\nAccuracy: {result2.box.map:.2f}')\n",
    "print(f'Scratch (5 epochs) \\nAccuracy: {result3.box.map:.2f}')  \n",
    "print(f'Scratch (15 epochs) \\nAccuracy: {result4.box.map:.2f}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
